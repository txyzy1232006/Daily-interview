# 深度学习 基础问题
## BP，Back-propagation（要能推导）
后向传播是在求解损失函数L对参数w求导时候用到的方法，目的是通过链式法则对参数进行一层一层的求导。这里重点强调：要将参数进行随机初始化而不是全部置0，否则所有隐层的数值都会与输入相关，这称为对称失效。

***
## 梯度消失、梯度爆炸
层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。
其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。

  + 预训练加微调
  + 梯度剪切 - 梯度爆炸
  + 权重正则化 - 梯度爆炸
  + Relu、Leaky-Relu等激活函数
  + Normalization
  + 残差结构 - 梯度消失
  + LSTM、GRU - 梯度消失

***
## 常用的激活函数
+ 非线性：当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=xf(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。基本上满足使大的数字更大，小的数字更小，强调特征，弱化非特征。

+ 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。

+ 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。

+ 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.

##### Sigmoid函数
sigmoid函数，也叫做S函数将值限定在(0,1)之间，能够很好的作为概率来解释算法得到的结果。函数定义为
$$sigmoid(x)=\frac{1}{1+e^{-x}}$$

其求导结果为
$$(sigmoid(x))'=\frac{e^{-x}}{(1+e^{-x})^2}=sigmoid(x)(1-sigmoid(x))$$


![](https://pic1.zhimg.com/80/v2-15ef91c7563ef2a046de444a904f1ff8_720w.jpg)
![](https://pic2.zhimg.com/80/v2-4b322e9c5d48a434c8a400d96a1de5fd_720w.jpg)

优点：
 >+ 函数将值限定在(0,1)之间, 能够很好的作为概率来解释算法得到的结果，适合作为二分类问题的输出。

缺点：
 >+ 梯度消失。梯度就对模型的更新没有任何贡献，导致网络就几乎不学习。
 >+ 不关于原点对称。这个特性会导致后面网络层的输入也不是零中心的，进而影响梯度下降的运作。
 >+ 计算exp比较耗时


##### tanh函数
tanh函数又称位双曲正切函数，其计算公式为
$$tanh(x)=\frac{sinh(x)}{cosh(x)}=\frac{\frac{e^z-e^{-z}}{2}}{\frac{e^z+e^{-z}}{2}}=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$
其求导结果为
$$(tanh(x))'=1-tanh^2(x)$$
![](https://pic2.zhimg.com/80/v2-eec4407568e4260a50cb01aad2cdb24d_720w.jpg)
![](https://pic2.zhimg.com/80/v2-4b322e9c5d48a434c8a400d96a1de5fd_720w.jpg)

优点：
 >+ 原点对称
 >+ 比sigmoid快

缺点：
 >+ 梯度消失



#### ReLU(Rectified Linear Unit 修正的线性单元)
ReLU是一个分段的函数，计算公式为：
$$\begin{equation} f(x)=\left\{ \begin{aligned} x & & if \quad x\geq0 \\0 & & if \quad x<0 \end{aligned} \right. \end{equation}$$

函数导数：
$$\begin{equation} f(x)’=\left\{ \begin{aligned} 1 & & if \quad x\geq0 \\0 & & if \quad x<0 \end{aligned} \right. \end{equation}$$
![](https://pic2.zhimg.com/80/v2-ff544a7f14011c174013ad51d2f0ceb1_720w.jpg)
![](https://pic1.zhimg.com/80/v2-c372e22a227f379218277879bdebd010_720w.jpg)

优点：
 >+ 解决梯度弥散问题，收敛加快
 >+ 计算简单

缺点：
 >+ 神经元可能“死掉”。一个非常大的梯度经过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了。如果这种情况发生，那么从此所有流过这个神经元的梯度将都变成 0。也就是说，这个 ReLU 单元在训练中将不可逆转的死亡，导致了数据多样化的丢失。实际中，如果学习率设置得太高，可能会发现网络中 40% 的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。合理设置学习率，会降低这种情况的发生概率。

#### leaky ReLU（带泄露的ReLU）
leaky ReLU是ReLU的一种变形，其计算公式为：
$$\begin{equation} f(x)=\left\{ \begin{aligned} x & & if \quad x\geq0 \\ \lambda x & & if \quad x<0 \end{aligned} \right. \end{equation}$$
其中 $\lambda$ 为（0,1）范围内的数。

函数的导数：
$$\begin{equation} f(x)'=\left\{ \begin{aligned} 1 & & if \quad x\geq0 \\ \lambda & & if \quad x<0 \end{aligned} \right. \end{equation}$$

![](https://pic2.zhimg.com/80/v2-0bdcd7ed12280b534a7e31b81008e4d5_720w.jpg)
![](https://pic3.zhimg.com/80/v2-b0477f500991ee7ec878e9429bfa4c1e_720w.jpg)

优点：
 >+ 解决神经元死亡问题

#### Maxout
Maxout 是对 ReLU 和 Leaky ReLU 的一般化归纳，其函数公式为
$${f_i}(x) = \mathop {\max }\limits_{j \in [1,k]} {z_{ij}}$$
$$其中, {z_{ij}} = {x^T}{W_{...ij}} + {b_{ij}},W \in {R^{d \times m \times d}}$$

假设 $w$ 是 2 维，那么有 $f(x) = \max ({w_1}^Tx + {b_1},{w_2}^Tx + {b_2})$ 。可以注意到，ReLU 和 Leaky ReLU 都是它的一个变形（比如，$w_1, b_1=0$ 的时候，就是 ReLU）。

与常规激活函数不同的是,它是一个可学习的分段线性函数.然而任何一个凸函数，都可以由线性分段函数进行逼近近似。Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个 maxout 节点就可以拟合任意的凸函数了（相减），前提是”隐隐含层”节点的个数可以任意多。实验结果表明Maxout与Dropout组合使用可以发挥比较好的效果。

优点：
 >+ 解决神经元死亡问题。
 >+ Maxout的拟合能力非常强，可以拟合任意的凸函数。

缺点：
 >+ 和 ReLU 对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。

#### Softmax函数
softmax函数与sigmoid函数很相似，也是将值变换到(0,1)之间。但是可以针对多个类别，预测出每个类别的归一化概率，所以通常softmax函数是在分类器的输出层使用。

其函数表达式为
$$S_i = \frac{e^{V_i}}{\sum_j{e^{V_j}}}$$

#### 如何选择激活函数
- 对于二分类问题，在输出层可以选择 sigmoid 函数。
- 对于多分类问题，在输出层可以选择 softmax 函数。
- 由于梯度消失问题，隐藏层不使用sigmoid函数和tanh。
- tanh函数由于以0为中心，通常性能会比sigmoid函数好。
- ReLU函数是一个通用的函数，一般在隐藏层都可以考虑使用。
***
## 损失函数
##### 对数似然函数
在分类问题中，我们一般使用最大似然估计（Maximum likelihood estimation）来构造损失函数。对于输入的 $x$，其对应的类标签为 $t$，我们的目标是找到这样的 $θ$ 使得 $p(t|x)$ 最大。在二分类的问题中，我们有：
> $p(t|x) = (y)^t(1-y)^{1-t}$
其中，$y=f(x)$ 是模型预测的概率值，$t$ 是样本对应的类标签

一般对似然函数取对数的负数，最小化对数似然：
> $-log\ p(t|x) = -log \prod_{i=1}^{C}y_i^{t_i} = -\sum_{i = i}^{C} t_{i} log(y_{i})$
##### 交叉熵

说交叉熵之前先介绍相对熵，相对熵又称为KL散度（Kullback-Leibler Divergence），用来衡量两个分布之间的距离，记为 $D_{KL}(p||q)$

> $\begin{split} D_{KL}(p||q) &= \sum_{x \in X} p(x) log \frac{p(x)}{q(x)} \\ & =\sum_{x \in X}p(x)log \ p(x) - \sum_{x \in X}p(x)log \ q(x) \\ & =-H(p) - \sum_{x \in X}p(x)log\ q(x) \end{split}$

这里 $H(p)$ 是 $p$的熵。假设有两个分布 $p$ 和 $q$，它们在给定样本集上的相对熵定义为：
> $CE(p, q) = -\sum_{x \in X}p(x)log\ q(x) = H(p) + D_{KL}(p||q)$

从这里可以看出，交叉熵和相对熵相差了 $H(p)$，而当 $p$已知的时候， $H(p)$ 是个常数，所以交叉熵和相对熵在这里是等价的，反映了分布 $p$ 和 $q$之间的相似程度。

对一个样本来说，真实类标签分布与模型预测的类标签分布可以用交叉熵来表示：
> $l_{CE} = -\sum_{i = 1}^{C}t_i log(y_i)$

对所有的样本n，我们有以下loss function：
> $L_{CE} = -\sum_{k = 1}^{n}\sum_{i = 1}^{C}t_{ki} log(y_{ki})$
其中 $t_{ki}$ 是样本 $k$ 属于类别 $i$ 的概率， $y_{ki}$ 是模型对样本 $k$ 预测为属于类别 $i$ 的概率。

softmax 使用交叉熵时，对权重求导：
> $\frac{\partial l_{CE}}{\partial w_j} = -\sum_{i = 1}^{C}\frac {\partial t_i log(y_i)}{\partial{w_j}} = -\sum_{i = 1}^{C}t_i \frac {\partial log(y_i)}{\partial{w_j}} = -\sum_{i = 1}^{C}t_i \frac{1}{y_i}\frac{\partial y_i}{\partial w_j}$

当 $i=j$ 时：
> $\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}} = \frac{ e^{a_i}\Sigma - e^{a_i}e^{a_j}}{\Sigma^2} =\frac{e^{a_i}}{\Sigma}\frac{\Sigma - e^{a_j}}{\Sigma} =y_i(1 - y_j)$

当 $i≠j$ 时：
> $\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}} = \frac{ 0 - e^{a_i}e^{a_j}}{\Sigma^2} =-\frac{e^{a_i}}{\Sigma}\frac{e^{a_j}}{\Sigma} =-y_iy_j$

将求导结果代入上式：
> $\begin{split} -\sum_{i = 1}^{C}t_i \frac{1}{y_i}\frac{\partial y_i}{\partial a_j} &= -\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_i} - \sum_{i \ne j}^{C} \frac{t_i}{y_i}\frac{\partial y_i}{\partial a_j} \\ & = -\frac{t_j}{y_i}y_i(1 - y_j) - \sum_{i \ne j}^{C} \frac{t_i}{y_i}(-y_iy_j) \\ & = -t_j + t_jy_j + \sum_{i \ne j}^{C}t_iy_j = -t_j + \sum_{i = 1}^{C}t_iy_j \\ & = -t_j + y_j\sum_{i = 1}^{C}t_i = y_j - t_j \end{split}$

***
## 神经网络训练技巧
[详解深度学习中的Normalization](https://zhuanlan.zhihu.com/p/33173246)
[BN和Dropout在训练和测试时的差别](https://zhuanlan.zhihu.com/p/61725100)
#### 神经网络训练时是否可以将全部参数初始化为0？



#### Normalization

**为什么需要 Normalization？**
+ 独立同分布与白化
+ 深度学习中的 Internal Covariate Shift

**Normalization 为什么会有效？**
+ 权重伸缩不变性
  >+ 权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。
  >+ 下层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，可以使用更高的学习率，提高网络的泛化性能。
+ 数据伸缩不变性
  >+ 数据的伸缩变化也不会影响到对权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。

**几种 Normalization**
+ Batch Normalization
  > BN，Batch Normalization，规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元在batch中所有输入值的均值和方差，可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。

  > **特点**
  > BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。

  > **BN训练和测试时的参数是一样的嘛？**
  >+ 在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。
  >+ 在测试时，进行一个样本的预测，用的均值和方差是全量训练数据的均值和方差。
  >+ 对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。

  > **BN训练时为什么不用全量训练集的均值和方差呢？**
  >+ 因为在训练的第一个完整epoch过程中是无法得到输入层之外其他层全量训练集的均值和方差，只能在前向传播过程中获取已训练batch的均值和方差。
  >+ 对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。
  >+ 但是一批数据和全量数据的均值和方差相差太多，又无法较好地代表训练集的分布，因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，去缩小与全量数据的差别。

+ Layer Normalization
  > LN 是一种横向的规范化，针对单个训练样本进行，它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。

  > **特点**
  > LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
  > 但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。


+ Weight Normalization
  > BN 和 LN 均将规范化应用于输入的特征数据 x ，而WN则另辟蹊径，将规范化应用于线性变换函数的权重 w。本质上都实现了对数据的规范化，只是用于 scale 的参数来源不同。

  > **特点**
  > WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构。

+ Cosine Normalization
  > CN 通过用余弦计算代替内积计算实现了规范化。

  > 原始的内积计算，其几何意义是 输入向量在权重向量上的投影，既包含 二者的夹角信息，也包含 两个向量的scale信息。去掉scale信息，可能导致表达能力的下降。



#### Dropout
Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。

> **Dropout 在训练和测试时都需要嘛？**
> Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。
>而在测试时，应该用整个训练好的模型，因此不需要dropout。

> **Dropout 如何平衡训练和测试时的差异呢？**
> Dropout ，在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0。
> 假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。
> 因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望。


***
## 过拟合的解决方法

+ 何为共线性，跟过拟合有啥关联
  >+ 多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
  >+ 共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。

+ Overfitting如何解决
  >+ 数据集增强 Data augmentation
  >+ 参数范数惩罚/正则化 Regularization
  >+ 模型集成 Model Ensemble 如BN, Dropout, weight decay


***
## 问题
1. 为什么Sigmoid和Tanh激活函数会导致梯度消失的现象？
> sigmiod和tanh的导数在$z=wx+b$很大或很小时都会趋近于0， 造成梯度消失的现象。
2. ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？
> 优点
 >- 计算简单。Sigmoid和Tanh激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值。
 >- ReLU的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。
 >- ReLU的单侧抑制提供了网络的稀疏表达能力。

> 缺点
 >- 神经元死亡

> 解决
 >- Leaky ReLU、Maxout、PReLU、elu
3. 为什么神经网络的损失函数是非凸的？
> 方法1：根据经典引理： 一个高维凸函数可以等价于无数个一维凸函数的叠加。一个（高维）函数是凸的，当且仅当把这个函数限制到任意直线上它在定义域上仍然是凸的。画图举例即可。

> 方法2： 只要二阶导再任意一点为负数，则不是凸函数。神经网络中任意方向的二阶导数为负，举例找出即可。

4. 深度学习的待学习的参数量和训练样本数量之间的关系
> 深层神经网络因为其结构，所以具有相较传统模型有很强的表达能力，从而也就需要更多的数据来避免过拟合的发生，以保证训练的模型在新的数据上也能有可以接受的表现。
>+ curse of dimensionality。深度学习往往用于高维空间的学习，但是随着维度的增高所需要的样本数呈指数性增长。

>+ 深度学习并没有足够的利用好函数本身的信息。
之所以深度学习这么流行，是因为他对于所学习的函数的限制非常少，几乎毫无任何假设（一个hidden layer的神经网络就可以估计所有的连续函数，而两个hidden layer则可以估计所有函数）。但是这也带来了一个缺憾，当函数足够smooth足够光滑的时候深度学习可能难以利用好这个信息。相反，local polynomial之类方法可以用更高次数的多项式来估计这个函数，利用好这个条件，达到相对较低的错误率。

>+ 深度学习常采用的是梯度下降。
梯度下降，加上并不那么高的learning rate，导致了在样本量有限的时候各个节点的参数变化有限。何况各个节点的参数已开始往往是随机的，如果运气不好+样本量有限，那么最后有不那么理想的错误率也是可想而知的。
