### 特征的归一化

##### 零均值归一化（z-score normalization）
将原始数据映射到均值为0，标准差为1的分布上。具体的方法是求出样本特征x的均值mean和标准差std，然后用（x-mean)/std来代替原特征。
在sklearn中，我们可以用StandardScaler来做z-score标准化。当然，如果我们是用pandas做数据预处理，可以自己在数据框里面减去均值，再除以方差，自己做z-score标准化。

##### 线性函数归一化（Min-Max Scaling）
对原始数据进行线性变换，预处理后使特征值映射到[0,1]之间。具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单。用(x-min)(b-a)/(max-min)+a来代替原特征即可。
在sklearn中，我们可以用MinMaxScaler来做max-min标准化。这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。

　　　　此外，经常我们还会用到中心化，主要是在PCA降维的时候，此时我们求出特征x的平均值mean后，用x-mean代替原特征，也就是特征的均值变成了0, 但是方差并不改变。这个很好理解，因为PCA就是依赖方差来降维的。


##### 为什么要归一化？
 + 要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。
 + 有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。
 + 有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。
 + 其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

虽然大部分机器学习模型都需要归一化，也有不少模型可以不做做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进。
***
### 异常特征样本清洗
##### 如何筛选掉异常特征样本？

 + 第一种是聚类，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。

 + 第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。

***
### 处理不平衡数据
##### 权重法
对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

##### 采样法

 + 对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当
 + 对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当

　　　　上述两种采样法有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差, 我们的训练数据假设不再是真实数据的无偏表述。所以有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本。在第一种方法中，我们浪费了很多数据。而第二类方法中有无中生有或者重复使用了数据，会导致过拟合的发生。

 + 阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可
 + 使用现有的集成学习分类器，如随机森林或者xgboost，并调整分类阈值

### 其他问题
1.  在对数据进行预处理时， 应该怎样处理类别型特征？
    > 序号编码（Ordinal Encoding） 、 独热编码（One-hot Encoding） 、 二进制编码
（Binary Encoding）
