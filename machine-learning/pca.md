# 降维
在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但对向维向量进行处理和分析时，会极大地消耗系统资源，甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。因此需要找到一种合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。

#### 降维的优点
+ 使得数据集更易使用。
+ 降低算法的计算开销。
+ 去除噪声。
+ 使得结果容易理解。

降维的算法有很多，比如奇异值分解(SVD)、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA)。

***
## 主成分分析 PCA
#### 如何定义主成分？
数据在某一个方向上分布得更为分散，这就意味着数据在这个方向上方差更大。在信号处理领域，我们认为信号具有较大方差， 噪声具有较小方差，信号与噪声之比称为信噪比。信噪比越大意味着数据的质量越好，反之，信噪比越小意味着数据
的质量越差。由此我们不难引出PCA的目标，**最大化投影方差**，也就是让数据在主轴上投影的方差最大。

数据投影后的方差就是协方差矩阵的特征值。 我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量。次佳投影方向位于最佳投影方向的正交空间中，是第二大特征值对应的特征向量，以此类推。

#### PCA降维准则
+ 最近重构性：样本集中所有点，重构后的点距离原来的点的误差之和最小。
+ 最大可分性：样本在低维空间的投影尽可能分开。

#### PCA算法流程
（1）对数据进行归一化处理，即每一位特征减去各自的平均值；
（2）计算归一化后的数据集的协方差矩阵；
（3）计算协方差矩阵的特征值与特征向量；
（4）对特征值从大到小排序；
（5）保留最重要的k个特征（通常k要小于n）；
（6）将$m \times n$的数据集乘以k个n维的特征向量的特征向量 $（n\times k）$，即最后降维的数据。

#### PCA算法缺点
+ 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。
+ 特征值分解有一些局限性，比如变换的矩阵必须是方阵。
+ 在非高斯分布情况下，PCA方法得出的主元可能并不是最优的。
+ PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA 的效果就大打折扣了。
+ PCA可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过 Kernel 函数将非线性相关转为线性相关。

#### PCA算法应用
（1）高维数据集的探索与可视化。

（2）数据压缩。

（3）数据预处理。

（4）图象、语音、通信的分析处理。

（5）降维(最主要)，去除数据冗余与噪声。


***
## SVD 奇异值分解

#### 矩阵知识
**正交与正定矩阵**
+ 正交矩阵：若一个方阵其行与列皆为正交的单位向量，则该矩阵为正交矩阵，且该矩阵的转置和其逆相等。两个向量正交的意思是两个向量的内积为 0。
+ 正定矩阵：如果对于所有的非零实系数向量 $z$，都有 $z^TAz>0$，则称矩阵 A 是正定的。正定矩阵的行列式必然大于 0， 所有特征值也必然 > 0。相对应的，半正定矩阵的行列式必然 ≥ 0。

**转置与共轭转置**
+ 转置: $A_{i,j}=A^T_{j,i}$
+ 共轭转置: 共轭转置只需要在转置的基础上，再叠加复数的共轭即可。$(A^H)_{i,j} = \overline{A_{j,i}}$

**酉矩阵**

酉矩阵是推广的标准正交矩阵，当酉矩阵中的元素均为实数时，酉矩阵实际就是正交矩阵。满足
$$U^{H}U=UU^{H}=I_{n}$$
由于
$$U^{-1}U=UU^{-1}=I_{n}$$
酉矩阵的逆矩阵，就是其共轭转置：
$$U^{-1}=U^{H}$$

**正规矩阵**
同酉矩阵一样，正规矩阵（normal matrix）也是一种特殊的方阵，它要求在矩阵乘法的意义下与它的共轭转置矩阵满足交换律。 即若 $A^H A =  A A^H$ ，则 $A$ 为正规矩阵。

**谱定理和谱分解（特征值分解）**
若矩阵 $A$ 是一个正规矩阵，则存在酉矩阵 $Q$，以及对角矩阵 $\Lambda$，使得 $A=Q\mathbf{\Lambda}Q^{-1}$ 。这也就是说，正规矩阵可经由酉变换，分解为对角矩阵；这种矩阵分解的方式，称为谱分解，又称特征值分解。

#### SVD 的定义
假设 $A$ 是一个 $m×n$阶矩阵，其中的元素全部属于域 $K$，也就是实数域或复数域。如此则存在一个分解使得
$$A=U\Sigma V^{H}$$
（H为共轭，另有用*号替代H。共轭代表在复数空间进行“数字共轭”和“转置”，所以在实数空间内，H表现为转置T。）

其中,
$U$ 是$m×m$阶酉矩阵，被称为左奇异矩阵。$U$ 的列是 $A^TA$ 的特征向量, 组成一套对 $A$ 的正交”输入”或”分析”的基向量;

$V^{H}$，即 $V$ 的共轭转置，是 $n×n$ 阶酉矩阵，$V$ 被称为右奇异矩阵。$V$ 的列是 $AA^T$ 的特征向量, 组成一套对 $A$ 的正交”输出”的基向量;

$Σ$ 是一个包含了奇异值的$m×$n阶非负实数对角矩阵；$Σ$ 对角线上的元素 $Σ_{i,i}$ 即为 $A$ 的奇异值，可视为是在输入与输出间进行的标量的”膨胀控制”。常见的做法是将奇异值由大而小排列。如此 $Σ$ 便能由 $A$ 唯一确定了。

#### SVD 的推导
现在，假设矩阵 $A_{m \times n}$ 的SVD分解是 $M=UΣV^H$；那么，我们有
$$AA^H = U\Sigma V^HVΣ^HU^H=U(ΣΣ^H)U^H \\ A^HA=VΣ^HU^HUΣV^H=V(Σ^HΣ)V^H$$
这也就是说，$U$ 的列向量（左奇异向量），是 $AA^H$ 的特征向量；同时，$V$ 的列向量（右奇异向量），是 $A^HA$ 的特征向量；另一方面，$A$ 的奇异值（$\lambda$ 的非零对角元素）则是 $AA^H$ 或者 $A^HA$ 的非零特征值的平方根。

#### 如何计算 SVD
1. 计算 $AA^H$ 和 $A^HA$；
2. 分别计算 $AA^H$ 和 $A^HA$ 的特征向量及其特征值；
3. $AA^H$ 的特征向量组成 $U$；而 $A^HA$ 的特征向量组成 $V$；
4. 对 $AA^H$ 和 $A^HA$ 的非零特征值求平方根，对应上述特征向量的位置，填入 $\lambda$ 的对角元。

#### SVD 压缩
 SVD 取 top k 就是压缩思想。
 $$A_{m \times n} = U_{m \times m} \cdot {\Sigma}_{m \times n} \cdot V_{n \times n}$$

 取 top k 个奇异值，则有
 $$A_{m \times n} \approx M_{m \times k} \cdot N_{k \times n}$$

#### SVD 与 PCA 的关系
PCA 中计算协方差矩阵 $X^TX$ 对 SVD 分解中使用的右奇异矩阵，PCA 可由 SVD 实现。
SVD 取 top k 就是压缩思想，PCA 中 $XX^T$ 实际是一种列压缩， $X^TX$是对行压缩。

#### SVD 在推荐召回中的实际使用
SVD 计算量大且要求数据稠密，但实际场景中不可能满足。解决方法：Funky SVD、LFM。

Funky SVD:
> $$\sum\limits_{i\in (有评分)} (y_i - U_i{\Sigma}V_j^T)^2$$

将奇异值融入左右奇异矩阵，可得隐语义模型 (Latent factor model, LFM):
> $$\sum\limits_{i\in (有评分)} (y_i - U_iV_j^T)^2$$
解决LFM的方法：
+ 梯度法
+ 交替最小二乘法 ALS

矩阵分解的好处：
+ 减少参数量
+ Embedding提高泛化性


## 线性判别分析（Linear Discriminant Analysis，LDA）
