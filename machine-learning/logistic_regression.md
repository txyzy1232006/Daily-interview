# Logistics Regression
# 1、直观解释
逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达到将数据二分类的目的。
核心公式：
对于给定的数据集$(x_i,y_i)^N_{i=1},y \in {0,1}$
$$ p(y=1|x)=\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$
$$ p(y=0|x)=\frac{1}{1+e^{w^T x+b}} $$
设$g(x)=p(y=1|x),1-g(x)=p(y=0|x)$
似然函数为
$$ \prod^N_{i=1} [g(x_i)] [1-g(x_i)]^{1-y_i} $$
对数似然函数为
$$ L(w)=\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_ilog \frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_i (w \ast x_i)-log(1+\exp(w \ast x_i))] $$
对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决
# 2、算法十问
1、LR 和线性回归的区别?
  >损失函数：线性模型是平方损失函数，而逻辑回归则是似然函数。LR是分类问题，线性回归是回归方法。

2、逻辑回归中为什么使用对数损失（对数极大似然）而不用平方损失?
 > 对于逻辑回归，这里所说的对数损失和极大似然是相同的。 不使用平方损失的原因是，在使用 Sigmoid 函数作为正样本的概率时，**同时将平方损失作为损失函数，这时所构造出来的损失函数是非凸的**，不容易求解，容易得到其局部最优解。 而如果使用极大似然，其目标函数就是对数似然函数，该损失函数是关于未知参数的高阶连续可导的凸函数，便于求其全局最优解。

3、为什么逻辑回归比线性回归要好？
 > 逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好

4、如果label={-1, +1}，给出LR的损失函数？
 > 假设label={-1,+1},则
$$p(y=1|x)=h_{\omega}(x) \tag{1}$$
$$p(y=-1 | x) = 1 - h_{\omega} (x)\tag{2}$$
对于sigmoid函数，有以下特性，
$$h(-x) = 1 - h(x)$$
所以（1）（2）式子可表示为
$$p(y|x) = h_\omega(yx)$$
同样，我们使用MLE作估计，
$$\begin{aligned}
L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  \prod_{i=1}^{m} h_\omega(y_i x_i)\\
&= \prod_{i=1}^{m} \frac{1}{1+e^{-y_iwx_i}}
\end{aligned}$$
对上式取对数及负值，得到损失为：
$$\begin{aligned}
-\log L(\omega)&= -\log \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log \frac{1}{1+e^{-y_iwx_i}}\\
&=  \sum_{i=1}^{m} \log(1+e^{-y_iwx_i})\\
\end{aligned}$$
即对于每一个样本，损失函数为：
$$L(\omega)=\log(1+e^{-y_iwx_i}) $$

5、LR为什么使用sigmoid函数？
 > https://blog.csdn.net/qq_19645269/article/details/79551576

6、LR如何进行并行计算？
 > http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html
 >
7、逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
 > 如果在损失函数最终收敛的情况下，有很多特征高度相关也不会影响分类器的效果。对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

8、带有L1正则项的逻辑LR如何进行参数更新？
 > 请参考西瓜书(周志华，机器学习)，P252

9、逻辑斯特回归是否要对特征进行离散化，以及为什么？
  > 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：离散特征的增加和减少都很容易，易于模型的快速迭代；稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

# 3、面试真题
1.LR为什么用sigmoid函数。这个函数有什么优点和缺点？为什么不用其他函数？
 > 由广义线性模型推得

2.LR的优点和缺点
优点：
 >+ 适合需要得到一个分类概率的场景。相比于linear regression而言，线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；LR和SVM克服了这个缺点，其中LR将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱；SVM直接去掉了远离分类决策面的数据，只考虑支持向量的影响。
 >+ 计算代价不高，容易理解实现。LR在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。
 >+ LR对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。（严重的多重共线性则可以使用逻辑回归结合L2正则化来解决，但是若要得到一个简约模型，L2正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。）

缺点：
>+ 容易欠拟合，分类精度不高
>+ 数据特征有缺失或者特征空间很大时表现效果并不好。

2.LR和SVM有什么不同吗
 > + LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
 > + 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。

 区别：
  > + LR是参数模型，SVM是非参数模型。
  > + 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
  > + SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
  > + 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
  > + LR能做的，SVM能做，但可能在准确率上有问题。SVM能做的LR有的做不了。

3.当用lr时，特征中的某些值很大，意味着这个特征重要程度很高？
> 这里的特征是指输入特征，输入特征和模型没有任何直接关系，其次lr模型需要的数据，一般情况下需要进行离散化，以保证模型的鲁棒性。但是一般情况下我们会根据lr中参数的大小来判断其对应特征的重要程度，在线性模型中（特征归一化之后）我们认为特征对应的参数值越大，其特征重要性越高。
